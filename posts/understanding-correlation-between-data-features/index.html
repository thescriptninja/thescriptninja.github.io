<!DOCTYPE html>
<html><head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Understanding correlation between data features - Parth Paradkar</title><meta name="viewport" content="width=device-width, initial-scale=1">

	<meta property="og:image" content=""/>
	<meta property="og:title" content="Understanding correlation between data features" />
<meta property="og:description" content="The Pearson correlation coefficient" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://parth-paradkar.github.io/posts/understanding-correlation-between-data-features/" />
<meta property="article:published_time" content="2020-01-20T01:23:54+05:30" />
<meta property="article:modified_time" content="2020-01-20T01:23:54+05:30" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding correlation between data features"/>
<meta name="twitter:description" content="The Pearson correlation coefficient"/>
<script src="https://parth-paradkar.github.io/js/feather.min.js"></script>
	
	<link href="https://parth-paradkar.github.io/css/fonts.css" rel="stylesheet">
	
	<link rel="stylesheet" type="text/css" media="screen" href="https://parth-paradkar.github.io/css/main.css" /><link rel="stylesheet" type="text/css" href="https://parth-paradkar.github.io/css/dark.css"  />
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://parth-paradkar.github.io/">Parth Paradkar</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/tags">Tags</a>
		
		<a href="/cv.pdf">CV</a>
		
		<a href="/about">About</a>
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">Understanding correlation between data features</h1>
			<div class="meta">Posted on Jan 20, 2020</div>
		</div>
		

		<section class="body">
			<h1 id="understanding-correlation">Understanding correlation</h1>
<p>One of the main questions I faced while exploring data was that how can I measure the strength of the relationship between two features of the data.</p>
<p>For example, how is the price of a car related with its mileage? How do we quantify their interdependence? How strong is the relationship?</p>
<p>I attempt to explore this mathematically in this notebook.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Importing the required libraries</span>
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> seaborn <span style="color:#f92672">as</span> sns
</code></pre></div><h3 id="we-shall-consider-a-dataset-that-gives-the-specifications-of-cars-and-their-prices">We shall consider a dataset that gives the specifications of cars and their prices.</h3>
<p>Dataset link: <a href="https://archive.ics.uci.edu/ml/datasets/automobile">https://archive.ics.uci.edu/ml/datasets/automobile</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;Automobile_data.csv&#34;</span>)

<span style="color:#75715e"># We shall consider three variables: the logarithm of the price, city mileage, curb weight.</span>
test_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;corr_data.csv&#39;</span>)
</code></pre></div><h2 id="the-pearson-correlation-coefficient">The Pearson Correlation Coefficient</h2>
<p>To measure the linear correlation between variables, correlation coefficients are defined. One such coefficient that pandas uses by default is the Pearson correlation coefficient.</p>
<p>Suppose we have two features (columns in the data) of the data: <code>X</code> and <code>Y</code>. Each feature contains <code>n</code> data points. Pearson correlation $r_{XY}$ is defined as:</p>
<p>$r_{XY} = \frac{\sum(X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum(X_i - \overline{X})^2}\sqrt{\sum(Y_i - \overline{Y})^2}}$</p>
<p>Here, $\overline{X}$ represents the mean of all the values of $X$.</p>
<p>A value of 1 represents positive linear dependence i.e. $Y_{i} = mX_{i} + C$ where $m &gt; 0$.</p>
<p>A value of -1 represents negative linear dependence i.e. $Y_{i} = mX_{i} + C$ where $m &lt; 0$</p>
<p>As we rarely find data that has exact linear dependence, the values of $r_{XY}$ practically lie between 1 and -1. Values closer to 0 represent little or no linear correlation between the two variables.</p>
<p>Let us investigate the different components of the above mathematical expression.</p>
<ol>
<li>$(X_{i} - \overline{X})$</li>
</ol>
<p>This expresses the distance of the $i^{th}$ point from $\overline{X}$. If $X_{i}$ is greater than the mean $X$ value, the expression&rsquo;s value will be positive and vice versa.</p>
<ol start="2">
<li>
<p>Similarly, $(Y_{i} - \overline{Y})$ is calculated.</p>
</li>
<li>
<p>$\sqrt{\sum(X_i - \overline{X})^2}$ and $\sqrt{\sum(Y_i - \overline{Y})^2}$</p>
</li>
</ol>
<p>The value of the numerator in the expression or $r_{XY}$ will depend upon the actual values of X and Y. Assume that we are dealing with values of $X$ and $Y$ that are quite high. We will correspondingly obtain high values of $X_{i} - \overline{X}$ and $Y_{i} - \overline{Y}$ and the value of the coefficient will not be constrained between 1 and -1.</p>
<p>The above expressions normalize the coefficient.</p>
<p>Let us head over to a practical example.</p>
<p>We&rsquo;ll consider three features of the data: <code>log_price</code>: which is the logarithm of the price of the car; <code>city-mpg</code>: the city mileage of cars; <code>curb-weight</code>: the total mass of the vehicle without any passengers or cargo.</p>
<p>P.S: We consider the logarithms of the price to deal with the skewness in the price and bring its distribution closer to the normal distribution.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test_df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>The correlation coefficients between each of the 3 features can be obtained from the corr() function offered by pandas</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">test_df<span style="color:#f92672">.</span>corr()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>I will try and give you a mathematical intuition of what is actually happening behind the scenes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Calculating the mean values of each column</span>

avgs <span style="color:#f92672">=</span> test_df<span style="color:#f92672">.</span>mean()
avgs
</code></pre></div><pre><code>log_price         9.350115
city-mpg         25.179104
curb-weight    2555.666667
dtype: float64
</code></pre>
<h2 id="consider-the-relationship-between-the-variables-log_price-and-curb_weight">Consider the relationship between the variables log_price and curb_weight</h2>
<p><img src="/img/pearson_correlation_12_1.png" alt="png"></p>
<p>Looking at the graph above, we find that the two variables have a strong positive linear interdependence. Higher the curb weight, higher is the price. This is also apparent from the correlation matrix above as the Pearson coefficient has a value of 0.891455.</p>
<p>The red point represents $(\overline{X}, \overline{Y})$.</p>
<p>Consider the two points shown in green and blue lying on opposite sides of the red point.</p>
<p>Let the green point be the $m^{th}$ point. Looking at its position, we can infer that $X_{m} - \overline{X} &gt; 0$ and $Y_{m} - \overline{Y} &gt; 0$.</p>
<p>Hence, $(X_{m} - \overline{X})(Y_{m} - \overline{Y}) &gt; 0$</p>
<p>Let the blue point be the $k^{th}$ point. From its position, we can say that $X_{m} - \overline{X} &lt; 0$ and $Y_{m} - \overline{Y} &lt; 0$.</p>
<p>Hence, $(X_{k} - \overline{X})(Y_{k} - \overline{Y}) &gt; 0$</p>
<p>Thus, for points lying to the <code>top-right</code> and <code>bottom-left</code> of $(\overline{X}, \overline{Y})$ will give positive values of the above product. The numerator of $r_{XY}$ is the sum of all such products. Hence for these two features, a net positive sum will be obtained, which will be normalized to values between 0 and 1.</p>
<h2 id="a-similar-analysis-can-be-done-for-city-mpg-and-log_price-in-the-graph-below">A similar analysis can be done for city-mpg and log_price in the graph below</h2>
<p><img src="/img/pearson_correlation_14_1.png" alt="png"></p>
<p>For the green($m^{th}$) point, $X_{m} - \overline{X} &gt; 0$ and $Y_{m} - \overline{Y} &lt; 0$.</p>
<p>Hence, $(X_{k} - \overline{X})(Y_{k} - \overline{Y}) &lt; 0$</p>
<p>For the blue($k^{th}$) point, $X_{m} - \overline{X} &lt; 0$ and $Y_{m} - \overline{Y} &gt; 0$.</p>
<p>Hence, $(X_{k} - \overline{X})(Y_{k} - \overline{Y}) &lt; 0$</p>
<p>For all points lying to the <code>top-left</code> and <code>bottom-right</code> will have negative values of the above product. As most points lie in these two regions, the net sum of these values will ultimately be negative and the value of $r_{XY}$ will lie between -1 and 0.</p>
<p>For these two features, we can see from the matrix that the correlation coefficient is -0.774933.</p>
<h2 id="regarding-the-strength-of-the-correlation-and-its-application">Regarding the strength of the correlation and its application.</h2>
<p>Often we need to choose the features to feed into a Regression model to predict a target value (in our example, the price of the car). Now that we know the correlation coefficients, we are in a better position to choose the best features for our model. Features having the highest magnitudes will be chosen since they have a linear relationship with the target variable. By manipulating the features, we can increase the accuracy of our model (for example, I chose the logarithm of the price since it handled the skewness and gave higher correlation values).</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>Using some simple mathematical techniques, we have simply divided the graph into four parts (or quadrants) about the mean point: <code>top-left</code> of the mean point, <code>top-right</code> of the mean point, <code>bottom-left</code> of the mean point and <code>bottom-right</code> of the mean point. We find the pair of quadrants in which majority of data points live and then infer the nature of the interdependence of the two variables under consideration.</p>
<p>The correlation matrix can be visualized using heatmaps in seaborn.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#f92672">.</span>heatmap(test_df<span style="color:#f92672">.</span>corr())
</code></pre></div><p><img src="/img/pearson_correlation_17_1.png" alt="png"></p>
<p>For a larger number of features, the correlation can be easily understood with a heatmap.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sns<span style="color:#f92672">.</span>heatmap(df<span style="color:#f92672">.</span>corr())
</code></pre></div><p><img src="/img/pearson_correlation_19_1.png" alt="png"></p>
<h3 id="thanks-for-reading">Thanks for reading!</h3>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/tech">tech</a></li>
					
				</ul>
			</nav>
			
			
		</div>
	</article>
	<script src="https://utteranc.es/client.js"
	        repo="parth-paradkar/parth-paradkar.github.io"
        	issue-term="pathname"
	        label="comment"
	        theme="github-dark"
        	crossorigin="anonymous"
        	async>
	</script>
</main>
<footer>
<hr><a class="soc" href="https://github.com/parth-paradkar" title="Github"><i data-feather="github"></i></a>|<a class="soc" href="https://twitter.com/parthparadkar_" title="Twitter"><i data-feather="twitter"></i></a>|<a class="soc" href="https://www.linkedin.com/in/parth-paradkar/" title="LinkedIn"><i data-feather="linkedin"></i></a>|⚡️
	2021  © Copyright notice |  <a href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
</footer>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-179536383-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>
      feather.replace()
</script></div>
    </body>
</html>
